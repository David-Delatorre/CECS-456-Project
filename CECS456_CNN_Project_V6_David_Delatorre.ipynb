{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/David-Delatorre/CECS-456-Project/blob/main/CECS456_CNN_Project_V6_David_Delatorre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uvXXg01oZs75"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from keras import backend as k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "DTk5QhAibh0D",
        "outputId": "49871fcc-f66f-4e8b-de20-42462fa5c55e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-a74e69e2fa0f>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Extract zip file from Google Drive to Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dataset'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract to /content/dataset folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# List the contents of the extracted folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0;31m# No need to compute the CRC if we don't have a reference value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expected_crc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Navigate to the path where the zip file is located\n",
        "zip_file_path = '/content/drive/MyDrive/medical-mnist.zip'\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Extract zip file from Google Drive to Colab\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset')  # Extract to /content/dataset folder\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "os.listdir('/content/dataset')\n",
        "\n",
        "# Define the dataset directory\n",
        "directory = \"/content/dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG2jorWeZs77"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pubi2wxYZs78"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_ds, test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory, # Directory where the data is located\n",
        "    labels='inferred', # labels are generated from the directory structure\n",
        "    label_mode='categorical', #labels are encoded as a categorical vector\n",
        "    class_names=None, #explicit list of class names in alphabetical order\n",
        "    color_mode='grayscale', #images will be converted to have 1 channel\n",
        "    batch_size=None, #Size of the batches of data.\n",
        "    image_size=(48, 48), #Size to resize images to after they are read (h, w)\n",
        "    shuffle=True, #Whether to shuffle the data.\n",
        "    seed=42, #Optional random seed for shuffling and transformations.\n",
        "    validation_split=0.2, #0<float<1, fraction of data to reserve for cv\n",
        "    subset='both', #returns a tuple of two datasets (training & cv)\n",
        "    interpolation='bilinear', #interpolation method used when resizing images\n",
        "    follow_links=False, #Whether to visit subdirectories pointed to by symlinks\n",
        "    crop_to_aspect_ratio=False, #resize the images with aspect ratio distortion\n",
        "    pad_to_aspect_ratio=True, #resize the images with aspect ratio distortion\n",
        "    data_format=None, #If None uses keras.config.image_data_format()\n",
        "    verbose=True #Whether to display no. info on classes & no. of files found\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpbv-UKJZs78"
      },
      "outputs": [],
      "source": [
        "#Displays the 6 class names\n",
        "print(train_ds.class_names)\n",
        "#Displays element specifications\n",
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEQ89MCDZs79"
      },
      "outputs": [],
      "source": [
        "#convert TensorFlow Datasets into NumPy format\n",
        "train_ds_np = tfds.as_numpy(train_ds)\n",
        "test_ds_np = tfds.as_numpy(test_ds)\n",
        "\n",
        "n_train = len(train_ds)\n",
        "n_test = len(test_ds)\n",
        "\n",
        "print(\"Number of training samples:\", n_train)\n",
        "print(\"Number of testing samples:\", n_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK2VIGxwZs79"
      },
      "outputs": [],
      "source": [
        "img_height, img_width, channels = 48, 48, 1\n",
        "num_classes = 6\n",
        "\n",
        "# Preallocate arrays\n",
        "X_train = np.zeros((n_train, img_height, img_width, channels))\n",
        "y_train = np.zeros((n_train, num_classes))\n",
        "X_test = np.zeros((n_test, img_height, img_width, channels))\n",
        "y_test = np.zeros((n_test, num_classes))\n",
        "\n",
        "# Fill the arrays\n",
        "for i, ele in enumerate(train_ds_np):\n",
        "    X_train[i] = ele[0]\n",
        "    y_train[i] = ele[1]\n",
        "\n",
        "for i, ele in enumerate(test_ds_np):\n",
        "    X_test[i] = ele[0]\n",
        "    y_test[i] = ele[1]\n",
        "\n",
        "# Verify shapes\n",
        "print(\"Shapes:\")\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "# Accessing specific labels\n",
        "print(\"Label of 421st train image:\", y_train[421])\n",
        "print(\"Label of 100th test image:\", y_test[100])\n",
        "\n",
        "#Expected Output:\n",
        "# Shapes:\n",
        "# (47164, 48, 48, 1) (47164, 6)\n",
        "# (11790, 48, 48, 1) (11790, 6)\n",
        "# Label of 421st train image: [0. 0. 1. 0. 0. 0.]\n",
        "# Label of 100th test image: [0. 0. 0. 0. 1. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rom_L0tiZs7-"
      },
      "source": [
        "# Data Preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCRcZoSlZs7-"
      },
      "outputs": [],
      "source": [
        "#normalizing the image data by converting the pixel values from the range [0, 255] (which is typical for 8-bit grayscale images) to the range [0, 1]\n",
        "X_train_normalized = X_train.astype('float32')\n",
        "X_test_normalized = X_test.astype('float32')\n",
        "X_train_normalized /= 255.0\n",
        "X_test_normalized /= 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvXida9YZs7-"
      },
      "source": [
        "# Model-1: CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V-FrYW4Zs7-"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from tensorflow.keras import datasets, layers, models, optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R5sUCVnZs7-"
      },
      "outputs": [],
      "source": [
        "my_model = models.Sequential([\n",
        "    #2D Convolutional Layer with 32 filters each of size 3x3\n",
        "    #Shape of the input images are 48x48 pixels with 1 channel\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    #Max Pooling Layer with a 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    #2D Convolutional Layer with 64 filters each of size 3x3\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    #Max Pooling Layer with a 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    #2D Convolutional Layer with 128 filters each of size 3x3\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    #This Layer flattens the 3D output of the last Conv2D layer into 1D array\n",
        "    layers.Flatten(),\n",
        "    #A fully connected (Dense) layer with 256 neurons\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    #Dense layer with 6 neurons corresponding to the 6 classes\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "my_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXbxi8-jZs7-"
      },
      "outputs": [],
      "source": [
        "my_model.compile(\n",
        "    optimizer = 'adam', #the learning rate defaults to 0.001\n",
        "    loss = 'categorical_crossentropy', #use this cross entropy function where are 2+ label classes in one-hot representation\n",
        "    #loss_weights=None,\n",
        "    metrics=['accuracy']\n",
        "    #weighted_metrics=None,\n",
        "    #run_eagerly=False,\n",
        "    #steps_per_execution=1,\n",
        "    #jit_compile='auto',\n",
        "    #auto_scale_loss=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyxXyZ8RZs7-"
      },
      "outputs": [],
      "source": [
        "#printing shapes and data types of training and testing sets\n",
        "print(X_train_normalized.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "print(X_train_normalized.dtype, y_train.dtype, X_test.dtype, y_test.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrojsuRYZs7-"
      },
      "outputs": [],
      "source": [
        "history = my_model.fit(\n",
        "    x=X_train_normalized,\n",
        "    y=y_train,\n",
        "    batch_size=None,\n",
        "    epochs=10,\n",
        "    #verbose=2,\n",
        "    #callbacks=None,\n",
        "    validation_split=0.2,\n",
        "    #validation_data = (X_test,y_test)\n",
        "    shuffle=True,\n",
        "    #class_weight=None,\n",
        "    #sample_weight=None,\n",
        "    #initial_epoch=0,\n",
        "    #steps_per_epoch=None,\n",
        "    #validation_steps=None,\n",
        "    #validation_batch_size=None,\n",
        "    #validation_freq=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5sG5Vc-Zs7-"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = my_model.evaluate(X_test_normalized, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTj_QJOXZs7-"
      },
      "source": [
        "Printing some predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s9PsXRtZs7-"
      },
      "outputs": [],
      "source": [
        "predictions = my_model.predict(X_test_normalized[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGQK9u3iZs7_"
      },
      "outputs": [],
      "source": [
        "predicted_labels = predictions.argmax(axis=1)\n",
        "actual_labels = y_test.argmax(axis=1)\n",
        "\n",
        "print(f\"Predicted Labels: {predicted_labels}\")\n",
        "\n",
        "print(f\"Actual Labels: {actual_labels[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 2**"
      ],
      "metadata": {
        "id": "DHyCQBQ7BVq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model2 = models.Sequential([\n",
        "    #2D Convolutional Layer with 32 filters each of size 3x3\n",
        "    #Shape of the input images are 48x48 pixels with 1 channel\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    #Max Pooling Layer with a 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    #2D Convolutional Layer with 64 filters each of size 3x3\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    #This Layer flattens the 3D output of the last Conv2D layer into 1D array\n",
        "    layers.Flatten(),\n",
        "    #A fully connected (Dense) layer with 256 neurons\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    #Dense layer with 6 neurons corresponding to the 6 classes\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "my_model2.summary()"
      ],
      "metadata": {
        "id": "jusnl3tTBLAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model2.compile(\n",
        "    optimizer = 'adam', #the learning rate defaults to 0.001\n",
        "    loss = 'categorical_crossentropy', #use this cross entropy function where are 2+ label classes in one-hot representation\n",
        "    #loss_weights=None,\n",
        "    metrics=['accuracy']\n",
        "    #weighted_metrics=None,\n",
        "    #run_eagerly=False,\n",
        "    #steps_per_execution=1,\n",
        "    #jit_compile='auto',\n",
        "    #auto_scale_loss=True\n",
        ")"
      ],
      "metadata": {
        "id": "4Z9vVnZZBqR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_model2.fit(\n",
        "    x=X_train_normalized,\n",
        "    y=y_train,\n",
        "    batch_size=None,\n",
        "    epochs=10,\n",
        "    #verbose=2,\n",
        "    #callbacks=None,\n",
        "    validation_split=0.2,\n",
        "    #validation_data = (X_test,y_test)\n",
        "    shuffle=True,\n",
        "    #class_weight=None,\n",
        "    #sample_weight=None,\n",
        "    #initial_epoch=0,\n",
        "    #steps_per_epoch=None,\n",
        "    #validation_steps=None,\n",
        "    #validation_batch_size=None,\n",
        "    #validation_freq=1\n",
        ")"
      ],
      "metadata": {
        "id": "BjHFs0haBtEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = my_model2.evaluate(X_test_normalized, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "eQoFQzliBv7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = my_model2.predict(X_test_normalized[:10])\n",
        "\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "actual_labels = y_test.argmax(axis=1)\n",
        "\n",
        "print(f\"Predicted Labels: {predicted_labels}\")\n",
        "\n",
        "print(f\"Actual Labels: {actual_labels[:10]}\")"
      ],
      "metadata": {
        "id": "UEvPNnEXByg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 3**"
      ],
      "metadata": {
        "id": "ObPrH7MICM6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model3 = models.Sequential([\n",
        "    #2D Convolutional Layer with 32 filters each of size 3x3\n",
        "    #Shape of the input images are 48x48 pixels with 1 channel\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    #Max Pooling Layer with a 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    #2D Convolutional Layer with 64 filters each of size 3x3\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    #Max Pooling Layer with a 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    #2D Convolutional Layer with 64 filters each of size 3x3\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    #This Layer flattens the 3D output of the last Conv2D layer into 1D array\n",
        "    layers.Flatten(),\n",
        "    #A fully connected (Dense) layer with 256 neurons\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    #Dense layer with 6 neurons corresponding to the 6 classes\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "my_model3.summary()"
      ],
      "metadata": {
        "id": "lt2h_75_B9Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model3.compile(\n",
        "    optimizer = 'adam', #the learning rate defaults to 0.001\n",
        "    loss = 'categorical_crossentropy', #use this cross entropy function where are 2+ label classes in one-hot representation\n",
        "    #loss_weights=None,\n",
        "    metrics=['accuracy']\n",
        "    #weighted_metrics=None,\n",
        "    #run_eagerly=False,\n",
        "    #steps_per_execution=1,\n",
        "    #jit_compile='auto',\n",
        "    #auto_scale_loss=True\n",
        ")"
      ],
      "metadata": {
        "id": "X8taSvuyCkyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_model3.fit(\n",
        "    x=X_train_normalized,\n",
        "    y=y_train,\n",
        "    batch_size=None,\n",
        "    epochs=10,\n",
        "    #verbose=2,\n",
        "    #callbacks=None,\n",
        "    validation_split=0.2,\n",
        "    #validation_data = (X_test,y_test)\n",
        "    shuffle=True,\n",
        "    #class_weight=None,\n",
        "    #sample_weight=None,\n",
        "    #initial_epoch=0,\n",
        "    #steps_per_epoch=None,\n",
        "    #validation_steps=None,\n",
        "    #validation_batch_size=None,\n",
        "    #validation_freq=1\n",
        ")"
      ],
      "metadata": {
        "id": "F4ZVfuBwCngM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = my_model3.evaluate(X_test_normalized, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "6vEGGboNCpr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = my_model3.predict(X_test_normalized[:10])\n",
        "\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "actual_labels = y_test.argmax(axis=1)\n",
        "\n",
        "print(f\"Predicted Labels: {predicted_labels}\")\n",
        "\n",
        "print(f\"Actual Labels: {actual_labels[:10]}\")"
      ],
      "metadata": {
        "id": "fDbJE9JSCsEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 4**"
      ],
      "metadata": {
        "id": "nltzVC71HU_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model4 = models.Sequential([\n",
        "    #2D Convolutional Layer with 32 filters each of size 3x3\n",
        "    #Shape of the input images are 48x48 pixels with 1 channel\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    #Max Pooling Layer with a 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    #This Layer flattens the 3D output of the last Conv2D layer into 1D array\n",
        "    layers.Flatten(),\n",
        "    #A fully connected (Dense) layer with 256 neurons\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    #Dense layer with 6 neurons corresponding to the 6 classes\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "my_model4.summary()"
      ],
      "metadata": {
        "id": "YWx4KOh3GPVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model4.compile(\n",
        "    optimizer = 'adam', #the learning rate defaults to 0.001\n",
        "    loss = 'categorical_crossentropy', #use this cross entropy function where are 2+ label classes in one-hot representation\n",
        "    #loss_weights=None,\n",
        "    metrics=['accuracy']\n",
        "    #weighted_metrics=None,\n",
        "    #run_eagerly=False,\n",
        "    #steps_per_execution=1,\n",
        "    #jit_compile='auto',\n",
        "    #auto_scale_loss=True\n",
        ")"
      ],
      "metadata": {
        "id": "_tPAyhqAGnT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_model4.fit(\n",
        "    x=X_train_normalized,\n",
        "    y=y_train,\n",
        "    batch_size=None,\n",
        "    epochs=10,\n",
        "    #verbose=2,\n",
        "    #callbacks=None,\n",
        "    validation_split=0.2,\n",
        "    #validation_data = (X_test,y_test)\n",
        "    shuffle=True,\n",
        "    #class_weight=None,\n",
        "    #sample_weight=None,\n",
        "    #initial_epoch=0,\n",
        "    #steps_per_epoch=None,\n",
        "    #validation_steps=None,\n",
        "    #validation_batch_size=None,\n",
        "    #validation_freq=1\n",
        ")"
      ],
      "metadata": {
        "id": "ylmZrWcFGpl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = my_model4.evaluate(X_test_normalized, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "La9EMOSoGsLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = my_model4.predict(X_test_normalized[:10])\n",
        "\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "actual_labels = y_test.argmax(axis=1)\n",
        "\n",
        "print(f\"Predicted Labels: {predicted_labels}\")\n",
        "\n",
        "print(f\"Actual Labels: {actual_labels[:10]}\")"
      ],
      "metadata": {
        "id": "KZ5ct7y7Gug-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 5**"
      ],
      "metadata": {
        "id": "2S8wI_odBdBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model5 = models.Sequential([\n",
        "    #2D Convolutional Layer with 32 filters each of size 3x3\n",
        "    #Shape of the input images are 48x48 pixels with 1 channel\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    #Max Pooling Layer with a 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    #This Layer flattens the 3D output of the last Conv2D layer into 1D array\n",
        "    layers.Flatten(),\n",
        "    #A fully connected (Dense) layer with 256 neurons\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    #Dense layer with 6 neurons corresponding to the 6 classes\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "my_model5.summary()"
      ],
      "metadata": {
        "id": "WxfCqySe-Nqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model5.compile(\n",
        "    optimizer = 'adam', #the learning rate defaults to 0.001\n",
        "    loss = 'categorical_crossentropy', #use this cross entropy function where are 2+ label classes in one-hot representation\n",
        "    #loss_weights=None,\n",
        "    metrics=['accuracy']\n",
        "    #weighted_metrics=None,\n",
        "    #run_eagerly=False,\n",
        "    #steps_per_execution=1,\n",
        "    #jit_compile='auto',\n",
        "    #auto_scale_loss=True\n",
        ")"
      ],
      "metadata": {
        "id": "-Gs6gMz5-S46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_model5.fit(\n",
        "    x=X_train_normalized,\n",
        "    y=y_train,\n",
        "    batch_size=None,\n",
        "    epochs=10,\n",
        "    #verbose=2,\n",
        "    #callbacks=None,\n",
        "    validation_split=0.2,\n",
        "    #validation_data = (X_test,y_test)\n",
        "    shuffle=True,\n",
        "    #class_weight=None,\n",
        "    #sample_weight=None,\n",
        "    #initial_epoch=0,\n",
        "    #steps_per_epoch=None,\n",
        "    #validation_steps=None,\n",
        "    #validation_batch_size=None,\n",
        "    #validation_freq=1\n",
        ")"
      ],
      "metadata": {
        "id": "VacUJ0k2-Wer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = my_model5.evaluate(X_test_normalized, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "HI4G9uJ0-Z5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = my_model5.predict(X_test_normalized[:10])\n",
        "\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "actual_labels = y_test.argmax(axis=1)\n",
        "\n",
        "print(f\"Predicted Labels: {predicted_labels}\")\n",
        "\n",
        "print(f\"Actual Labels: {actual_labels[:10]}\")"
      ],
      "metadata": {
        "id": "hiDdiqBF-dgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 6**"
      ],
      "metadata": {
        "id": "4daviSgOEol7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model6 = models.Sequential([\n",
        "    #2D Convolutional Layer with 32 filters each of size 3x3\n",
        "    #Shape of the input images are 48x48 pixels with 1 channel\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    #Max Pooling Layer with a 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    #2D Convolutional Layer with 64 filters each of size 3x3\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    #Max Pooling Layer with a 2x2 pool size\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    #2D Convolutional Layer with 64 filters each of size 3x3\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    #This Layer flattens the 3D output of the last Conv2D layer into 1D array\n",
        "    layers.Flatten(),\n",
        "    #A fully connected (Dense) layer with 256 neurons\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    #Dense layer with 6 neurons corresponding to the 6 classes\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "my_model6.summary()"
      ],
      "metadata": {
        "id": "Rk7Gon6lBp--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model6.compile(\n",
        "    optimizer = 'adam', #the learning rate defaults to 0.001\n",
        "    loss = 'categorical_crossentropy', #use this cross entropy function where are 2+ label classes in one-hot representation\n",
        "    #loss_weights=None,\n",
        "    metrics=['accuracy']\n",
        "    #weighted_metrics=None,\n",
        "    #run_eagerly=False,\n",
        "    #steps_per_execution=1,\n",
        "    #jit_compile='auto',\n",
        "    #auto_scale_loss=True\n",
        ")"
      ],
      "metadata": {
        "id": "vhB-Km3JBvo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_model6.fit(\n",
        "    x=X_train_normalized,\n",
        "    y=y_train,\n",
        "    batch_size=None,\n",
        "    epochs=10,\n",
        "    #verbose=2,\n",
        "    #callbacks=None,\n",
        "    validation_split=0.2,\n",
        "    #validation_data = (X_test,y_test)\n",
        "    shuffle=True,\n",
        "    #class_weight=None,\n",
        "    #sample_weight=None,\n",
        "    #initial_epoch=0,\n",
        "    #steps_per_epoch=None,\n",
        "    #validation_steps=None,\n",
        "    #validation_batch_size=None,\n",
        "    #validation_freq=1\n",
        ")"
      ],
      "metadata": {
        "id": "ydb3p6SwBy96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = my_model6.evaluate(X_test_normalized, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "4QhImyrTB7Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = my_model6.predict(X_test_normalized[:10])\n",
        "\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "actual_labels = y_test.argmax(axis=1)\n",
        "\n",
        "print(f\"Predicted Labels: {predicted_labels}\")\n",
        "\n",
        "print(f\"Actual Labels: {actual_labels[:10]}\")"
      ],
      "metadata": {
        "id": "tuBaC55sB-C2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 614679,
          "sourceId": 1099232,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30715,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}